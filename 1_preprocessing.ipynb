{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(file_dir, data):\n",
    "    f = open(file_dir,\"wb\")\n",
    "    pickle.dump(data, f, protocol=4)\n",
    "    f.close()\n",
    "    \n",
    "def read_pkl(file_dir):\n",
    "    f = open(file_dir,\"rb\")\n",
    "    data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './London/London_historical_meo_grid_extend.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8100/2322093441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./London/London_historical_meo_grid_extend.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mGrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './London/London_historical_meo_grid_extend.csv'"
     ]
    }
   ],
   "source": [
    "Grid = pd.read_csv(\"./London/London_historical_meo_grid_extend.csv\")\n",
    "Grid.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api_data</th>\n",
       "      <th>need_prediction</th>\n",
       "      <th>historical_data</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SiteType</th>\n",
       "      <th>SiteName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ST5</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.389287</td>\n",
       "      <td>-0.141662</td>\n",
       "      <td>Industrial</td>\n",
       "      <td>Sutton - Beddington Lane north</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.410039</td>\n",
       "      <td>-0.127523</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>Croydon - Norbury Manor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TD5</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.425256</td>\n",
       "      <td>-0.345608</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Richmond Upon Thames - Bushy Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GR4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.452580</td>\n",
       "      <td>0.070766</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Greenwich - Eltham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.456300</td>\n",
       "      <td>0.085606</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Greenwich and Bexley - Falconwood FDMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GR9</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.456357</td>\n",
       "      <td>0.040725</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Greenwich - Westhorne Avenue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BX9</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.465983</td>\n",
       "      <td>0.184877</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Bexley - Slade Green FDMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BX1</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.465983</td>\n",
       "      <td>0.184877</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Bexley - Slade Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LW2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.474954</td>\n",
       "      <td>-0.039641</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Lewisham - New Cross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GN3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.486957</td>\n",
       "      <td>0.095111</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Greenwich - Plumstead High Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LH0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.488780</td>\n",
       "      <td>-0.441627</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>Hillingdon - Harlington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GN0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.490532</td>\n",
       "      <td>0.074003</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Greenwich - A206 Burrage Grove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CT3</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.513847</td>\n",
       "      <td>-0.077766</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>City of London - Sir John Cass School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CT2</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.514525</td>\n",
       "      <td>-0.104516</td>\n",
       "      <td>Kerbside</td>\n",
       "      <td>City of London - Farringdon Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TH4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.515046</td>\n",
       "      <td>-0.008418</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Tower Hamlets - Blackwall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HV1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.520787</td>\n",
       "      <td>0.205461</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Havering - Rainham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KC1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.521047</td>\n",
       "      <td>-0.213492</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>Kensington and Chelsea - North Ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KF1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.521047</td>\n",
       "      <td>-0.213492</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>Kensington and Chelsea - North Ken FIDAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BL0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.522287</td>\n",
       "      <td>-0.125848</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>Camden - Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MY7</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.522540</td>\n",
       "      <td>-0.154590</td>\n",
       "      <td>Kerbside</td>\n",
       "      <td>Westminster - Marylebone Road FDMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD9</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.527707</td>\n",
       "      <td>-0.129053</td>\n",
       "      <td>Roadside</td>\n",
       "      <td>Camden - Euston Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51.544219</td>\n",
       "      <td>-0.175284</td>\n",
       "      <td>Kerbside</td>\n",
       "      <td>Camden - Swiss Cottage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB7</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.569484</td>\n",
       "      <td>0.082907</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>Redbridge - Ley Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HR1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>51.617327</td>\n",
       "      <td>-0.298775</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>Harrow - Stanmore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    api_data need_prediction  historical_data   Latitude  Longitude  \\\n",
       "ST5     True            True             True  51.389287  -0.141662   \n",
       "CR8      NaN             NaN             True  51.410039  -0.127523   \n",
       "TD5     True             NaN             True  51.425256  -0.345608   \n",
       "GR4     True            True             True  51.452580   0.070766   \n",
       "GB0      NaN             NaN             True  51.456300   0.085606   \n",
       "GR9     True            True             True  51.456357   0.040725   \n",
       "BX9     True             NaN             True  51.465983   0.184877   \n",
       "BX1     True             NaN             True  51.465983   0.184877   \n",
       "LW2     True            True             True  51.474954  -0.039641   \n",
       "GN3     True            True             True  51.486957   0.095111   \n",
       "LH0      NaN             NaN             True  51.488780  -0.441627   \n",
       "GN0     True            True             True  51.490532   0.074003   \n",
       "CT3     True             NaN             True  51.513847  -0.077766   \n",
       "CT2     True             NaN             True  51.514525  -0.104516   \n",
       "TH4     True            True             True  51.515046  -0.008418   \n",
       "HV1     True            True             True  51.520787   0.205461   \n",
       "KC1      NaN             NaN             True  51.521047  -0.213492   \n",
       "KF1     True            True             True  51.521047  -0.213492   \n",
       "BL0     True            True             True  51.522287  -0.125848   \n",
       "MY7     True            True             True  51.522540  -0.154590   \n",
       "CD9     True            True             True  51.527707  -0.129053   \n",
       "CD1     True            True             True  51.544219  -0.175284   \n",
       "RB7     True             NaN             True  51.569484   0.082907   \n",
       "HR1      NaN             NaN             True  51.617327  -0.298775   \n",
       "\n",
       "             SiteType                                  SiteName  \n",
       "ST5        Industrial            Sutton - Beddington Lane north  \n",
       "CR8  Urban Background                   Croydon - Norbury Manor  \n",
       "TD5          Suburban         Richmond Upon Thames - Bushy Park  \n",
       "GR4          Suburban                        Greenwich - Eltham  \n",
       "GB0          Roadside    Greenwich and Bexley - Falconwood FDMS  \n",
       "GR9          Roadside              Greenwich - Westhorne Avenue  \n",
       "BX9          Suburban                 Bexley - Slade Green FDMS  \n",
       "BX1          Suburban                      Bexley - Slade Green  \n",
       "LW2          Roadside                      Lewisham - New Cross  \n",
       "GN3          Roadside         Greenwich - Plumstead High Street  \n",
       "LH0  Urban Background                   Hillingdon - Harlington  \n",
       "GN0          Roadside            Greenwich - A206 Burrage Grove  \n",
       "CT3  Urban Background     City of London - Sir John Cass School  \n",
       "CT2          Kerbside        City of London - Farringdon Street  \n",
       "TH4          Roadside                 Tower Hamlets - Blackwall  \n",
       "HV1          Roadside                        Havering - Rainham  \n",
       "KC1  Urban Background        Kensington and Chelsea - North Ken  \n",
       "KF1  Urban Background  Kensington and Chelsea - North Ken FIDAS  \n",
       "BL0  Urban Background                       Camden - Bloomsbury  \n",
       "MY7          Kerbside        Westminster - Marylebone Road FDMS  \n",
       "CD9          Roadside                      Camden - Euston Road  \n",
       "CD1          Kerbside                    Camden - Swiss Cottage  \n",
       "RB7  Urban Background                    Redbridge - Ley Street  \n",
       "HR1  Urban Background                         Harrow - Stanmore  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stations = pd.read_csv(\"London/London_AirQuality_Stations.csv\", index_col=0)\n",
    "Stations.sort_values(by=['Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api_data</th>\n",
       "      <th>need_prediction</th>\n",
       "      <th>historical_data</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SiteType</th>\n",
       "      <th>SiteName</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tag</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>57.046927</td>\n",
       "      <td>9.931002</td>\n",
       "      <td>Bybaggrundsstation</td>\n",
       "      <td>Oesterbro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gade</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>57.050974</td>\n",
       "      <td>9.916690</td>\n",
       "      <td>Gadestation</td>\n",
       "      <td>Vesterbro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      api_data  need_prediction  historical_data   Latitude  Longitude  \\\n",
       "id                                                                       \n",
       "Tag        NaN             True             True  57.046927   9.931002   \n",
       "Gade       NaN             True             True  57.050974   9.916690   \n",
       "\n",
       "                SiteType   SiteName  \n",
       "id                                   \n",
       "Tag   Bybaggrundsstation  Oesterbro  \n",
       "Gade         Gadestation  Vesterbro  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stations = pd.read_csv(\"Aal/Aalborg_AQ_Stations.csv\", index_col=0)\n",
    "Stations.sort_values(by=['Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gade is 1\n",
      "['Tag']\n",
      "[0.01487318234272708]\n",
      "Tag is 1\n",
      "['Gade']\n",
      "[0.01487318234272708]\n"
     ]
    }
   ],
   "source": [
    "# The purpose of this part is to find the neighbour stations of each station\n",
    "\n",
    "# import math\n",
    "station_position = {}\n",
    "station_name = Stations.index.values\n",
    "station_Latitude = np.asarray(Stations['Latitude'].values)\n",
    "station_Longitude = np.asarray(Stations['Longitude'].values)\n",
    "for i in range(len(station_name)):  \n",
    "    name = station_name[i]\n",
    "    if name!= 'KC1' and name!= 'BX9' and name!= 'CT2':\n",
    "        point_list = []\n",
    "        point_dist = []\n",
    "        temp_name_0 = []\n",
    "        temp_dist_0 = []\n",
    "        temp_name_1 = []\n",
    "        temp_dist_1 = []\n",
    "        temp_name_2 = []\n",
    "        temp_dist_2 = []\n",
    "        temp_name_3 = []\n",
    "        temp_dist_3 = []\n",
    "        # angle_list = []\n",
    "        dist_Latitude = station_Latitude - station_Latitude[i]\n",
    "        dist_Longitude = station_Longitude - station_Longitude[i]\n",
    "        dist_list = np.sqrt(np.square(dist_Latitude) + np.square(dist_Longitude))\n",
    "        for j in range(len(station_name)):\n",
    "            if j!=i and station_name[j] != 'BX9' and station_name[j] != 'KC1' and station_name[j] != 'CT2':\n",
    "                # if  dist_list[j]< 0.2:\n",
    "                    # tan_list.append(dist_Latitude[j]/dist_Longitude[j])\n",
    "                    lat = dist_Latitude[j]\n",
    "                    lon = dist_Longitude[j]\n",
    "                    if(lat>=0 and lon>=0):\n",
    "                        temp_name_0.append(station_name[j])\n",
    "                        temp_dist_0.append(dist_list[j])\n",
    "                    elif(lat>=0 and lon<0):\n",
    "                        temp_name_1.append(station_name[j])\n",
    "                        temp_dist_1.append(dist_list[j])      \n",
    "                    elif(lat<0 and lon<0):\n",
    "                        temp_name_2.append(station_name[j])\n",
    "                        temp_dist_2.append(dist_list[j])\n",
    "                    elif(lat<0 and lon>=0):\n",
    "                        temp_name_3.append(station_name[j])\n",
    "                        temp_dist_3.append(dist_list[j])\n",
    "                    # angle_list.append(math.atan(dist_Latitude[j]/dist_Longitude[j]))\n",
    "                    # print(station_name[j],dist_list[j],math.atan(dist_Latitude[j]/dist_Longitude[j]))\n",
    "        if(len(temp_name_0)):\n",
    "            j = temp_dist_0.index(min(temp_dist_0))\n",
    "            point_list.append(temp_name_0[j])\n",
    "            point_dist.append(temp_dist_0[j])\n",
    "        if(len(temp_name_1)):\n",
    "            j = temp_dist_1.index(min(temp_dist_1))\n",
    "            point_list.append(temp_name_1[j])\n",
    "            point_dist.append(temp_dist_1[j])\n",
    "        if(len(temp_name_2)):\n",
    "            j = temp_dist_2.index(min(temp_dist_2))\n",
    "            point_list.append(temp_name_2[j])\n",
    "            point_dist.append(temp_dist_2[j])\n",
    "        if(len(temp_name_3)):\n",
    "            j = temp_dist_3.index(min(temp_dist_3))\n",
    "            point_list.append(temp_name_3[j])\n",
    "            point_dist.append(temp_dist_3[j])\n",
    "        print(name, 'is', len(point_list))\n",
    "        # angle_list = np.arctan(tan_list)\n",
    "        # print(angle_list)\n",
    "        print(point_list)\n",
    "        print(point_dist)\n",
    "        station_position[name] = [station_Latitude[i],station_Longitude[i],point_list,point_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gade\n",
      "[57.050974, 9.91669, ['Tag'], [0.01487318234272708]]\n",
      "Tag\n",
      "[57.046927, 9.931002, ['Gade'], [0.01487318234272708]]\n"
     ]
    }
   ],
   "source": [
    "# print(station_position)\n",
    "for name in station_position.keys():\n",
    "    print(name)\n",
    "    print(station_position[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(\"Aal/station_processing.pkl\", station_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_position = read_pkl(\"Aal/station_processing.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'London/London_historical_aqi_forecast_stations_20180331.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19844/2577318461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# the NaN situation of the first original air quality csv file provided by the KDD website\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mAirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"London/London_historical_aqi_forecast_stations_20180331.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmissing_values_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAirs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmissing_values_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'London/London_historical_aqi_forecast_stations_20180331.csv'"
     ]
    }
   ],
   "source": [
    "# the NaN situation of the first original air quality csv file provided by the KDD website\n",
    "\n",
    "Airs = pd.read_csv(\"Aal/London_historical_aqi_forecast_stations_20180331.csv\")\n",
    "missing_values_count = Airs.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Airs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8100/3178657124.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAirs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Airs' is not defined"
     ]
    }
   ],
   "source": [
    "Airs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nobody\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3172: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Station_ID             22959\n",
       "MeasurementDateGMT     22958\n",
       "PM2.5 (ug/m3)          53621\n",
       "PM10 (ug/m3)          102438\n",
       "NO2 (ug/m3)            91703\n",
       "Unnamed: 5            141633\n",
       "Unnamed: 6            141633\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the NaN situation of the second original air quality csv file provided by the KDD website\n",
    "\n",
    "Airs = pd.read_csv(\"London/London_historical_aqi_other_stations_20180331.csv\")\n",
    "missing_values_count = Airs.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gade\n",
      "Tag\n"
     ]
    }
   ],
   "source": [
    "# The purpose of this part is to merge two original air quality csv files provided by the KDD website\n",
    "\n",
    "df = pd.read_csv(\"Aal/GT.csv\")\n",
    "df = df[[\"Recorded\",\"station_id\",\"NO2\",\"NOx\"]]\n",
    "df.rename(columns={'NO2': 'NO2_Concentration',\n",
    "                   'NOx': 'NOx_Concentration',\n",
    "                   'Recorded': 'time'\n",
    "                  }, inplace=True)\n",
    "group_ = df.groupby(\"station_id\")\n",
    "for name, g in group_:\n",
    "    print(name)\n",
    "\n",
    "# df1 = pd.read_csv(\"London/London_historical_aqi_other_stations_20180331.csv\")\n",
    "# df1.rename(columns={'NO2 (ug/m3)': 'NO2_Concentration',\n",
    "#                     'PM10 (ug/m3)': 'PM10_Concentration',\n",
    "#                     'PM2.5 (ug/m3)': 'PM25_Concentration',\n",
    "#                     'MeasurementDateGMT': 'time',\n",
    "#                     'Station_ID': 'station_id'\n",
    "#                    }, inplace=True)\n",
    "# df = pd.concat([df, df1])\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.index = df['time']\n",
    "df['time_week'] = df.index.map(lambda x: x.weekday)\n",
    "df['time_year'] = df.index.map(lambda x: x.year)\n",
    "df['time_month'] = df.index.map(lambda x: x.month)\n",
    "df['time_day'] = df.index.map(lambda x: x.day)\n",
    "df['time_hour'] = df.index.map(lambda x: x.hour)\n",
    "Airs = df[[\"station_id\", \"NO2_Concentration\", \"NOx_Concentration\",\n",
    "           'time_year', 'time_month', 'time_week', 'time_day', 'time_hour']]\n",
    "\n",
    "Airs = Airs.sort_index()\n",
    "group = Airs.groupby(\"station_id\")\n",
    "# print(group)\n",
    "station_group = {}\n",
    "for name, g in group:\n",
    "    if name != 'CT2':\n",
    "        station_group[name] = g.sort_index()[:39464]\n",
    "#         print(name)\n",
    "#         print(len(station_group[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gade\n",
      "0\n",
      "0\n",
      "Tag\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# display the negative value count\n",
    "# padding the negative value into zero\n",
    "attr_need = [\"NO2_Concentration\", \"NOx_Concentration\"]\n",
    "a = []\n",
    "for name in station_group:\n",
    "    print(name)\n",
    "    df = station_group[name][attr_need]\n",
    "    #a.append()\n",
    "    print(sum(n < 0 for n in df.values.flatten()))\n",
    "    station_group[name][attr_need] = df.clip(lower=0)\n",
    "    df = station_group[name][attr_need]\n",
    "    print(sum(n < 0 for n in df.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'KF1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8100/2125375327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mattr_need\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"NO2_Concentration\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NOx_Concentration\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mduplicate_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstation_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_need\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"KC1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"KF1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mduplicate_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstation_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_need\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"BX9\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"BX1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8100/2125375327.py\u001b[0m in \u001b[0;36mduplicate_padding\u001b[1;34m(station_group, attr_need, remove_station, keep_station)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mduplicate_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstation_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_need\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_station\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_station\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mkeep_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstation_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_station\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr_need\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mkeep_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mremove_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstation_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremove_station\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr_need\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'KF1'"
     ]
    }
   ],
   "source": [
    "# The purpose of this part is to merge two duplicated station pairs\n",
    "\n",
    "def duplicate_padding(station_group, attr_need, remove_station, keep_station):\n",
    "    keep_ = station_group[keep_station][attr_need]\n",
    "    keep_values = keep_.values\n",
    "    remove_ = station_group[remove_station][attr_need]\n",
    "    remove_values = remove_.values\n",
    "    print (keep_values.shape, remove_values.shape)\n",
    "    print(keep_.isnull().sum())\n",
    "    print(remove_.isnull().sum())\n",
    "    for i in range(remove_values.shape[0]):\n",
    "        for j in range(remove_values.shape[1]):\n",
    "            if np.isnan(keep_values[i, j]):\n",
    "                keep_values[i, j] = remove_values[i, j]\n",
    "    station_group[keep_station].loc[:, attr_need] = keep_values\n",
    "    keep_ = station_group[keep_station][attr_need]\n",
    "    print(keep_.isnull().sum())\n",
    "    print(len(station_group))\n",
    "    station_group.pop(remove_station)\n",
    "    print(len(station_group))\n",
    "    \n",
    "attr_need = [\"NO2_Concentration\", \"NOx_Concentration\"]\n",
    "duplicate_padding(station_group, attr_need, \"KC1\", \"KF1\")\n",
    "duplicate_padding(station_group, attr_need, \"BX9\", \"BX1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2352443608470127 0.8912853213303326\n",
      "0.23783715066543912 0.8850931677018633\n",
      "0.2793970525986878 0.9356091247370976\n",
      "0.18661335342386087 0.8498766954377311\n",
      "Gade\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "Tag\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# padding the NaN value with neighbour values and historical values\n",
    "\n",
    "def neighbour_padding(station_group, attr_need, station_position):\n",
    "    normal_change_health = []# the change% when the previous value is smaller than 20\n",
    "    normal_change_else = []# the change% when the previous value is bigger than 20\n",
    "    for j in range(len(attr_need)):# calculate the change% in the normal situation \n",
    "        change_list_health = []\n",
    "        change_list_else = []\n",
    "        for name in station_position.keys():\n",
    "            keep_values = station_group[name][attr_need].values\n",
    "            pre_nan = True\n",
    "            for i in range(keep_values.shape[0]):\n",
    "                if np.isnan(keep_values[i, j]):\n",
    "                    this_nan = True\n",
    "                else:\n",
    "                    if(not pre_nan):\n",
    "                        pre_value = keep_values[i-1, j]\n",
    "                        this_value = keep_values[i, j]\n",
    "                        if(pre_value>0 and this_value>0):\n",
    "                            change_percent = (this_value - pre_value) / pre_value\n",
    "                            if(change_percent<0):\n",
    "                                change_percent = - change_percent\n",
    "                            if(pre_value<20):\n",
    "                                change_list_health.append(change_percent)\n",
    "                            else:\n",
    "                                change_list_else.append(change_percent)\n",
    "                    this_nan = False\n",
    "                pre_nan = this_nan\n",
    "        total_number = len(change_list_health)\n",
    "        average = sum(change_list_health)/total_number\n",
    "        normal_number = sum(i < average*2  for i in change_list_health)\n",
    "        # print(attr_need[j],total_number,max(change_list),min(change_list),normal_number)\n",
    "        print(average,normal_number/total_number)\n",
    "        normal_change_health.append(normal_number*2)    \n",
    "        \n",
    "        total_number = len(change_list_else)\n",
    "        average = sum(change_list_else)/total_number\n",
    "        normal_number = sum(i < average*2  for i in change_list_else)\n",
    "        # print(attr_need[j],total_number,max(change_list),min(change_list),normal_number)\n",
    "        print(average,normal_number/total_number)\n",
    "        normal_change_else.append(normal_number*2) \n",
    "    \n",
    "    for name in station_position.keys():# padding the nan value\n",
    "        print(name)\n",
    "        neighbour_list = station_position[name][2]\n",
    "        neighbour_dist = station_position[name][3]\n",
    "        # print(neighbour_list)\n",
    "        # print(neighbour_dist)\n",
    "        \n",
    "        keep_ = station_group[name][attr_need]\n",
    "        print(keep_.isnull().sum())\n",
    "        keep_values = keep_.values           \n",
    "        neighbour_values_list = []\n",
    "        for neighbour in neighbour_list:\n",
    "            neighbour_values_list.append(station_group[neighbour][attr_need].values)\n",
    "\n",
    "        for j in range(keep_values.shape[1]):\n",
    "            pre_nan = True\n",
    "            for i in range(keep_values.shape[0]):\n",
    "                if np.isnan(keep_values[i, j]):# this value is empty\n",
    "                    this_nan = True\n",
    "                    temp_list = []\n",
    "                    temp_dist = []\n",
    "                    for k in range(len(neighbour_values_list)):# find neighbour values which are no empty\n",
    "                        if(not np.isnan(neighbour_values_list[k][i, j])):\n",
    "                            temp_list.append(neighbour_values_list[k][i, j])\n",
    "                            temp_dist.append(neighbour_dist[k])\n",
    "                    if(len(temp_list)):# if exists neighbour values which are no empty\n",
    "                        temp_normal = min(temp_dist) + max(temp_dist)\n",
    "                        temp_result = 0\n",
    "                        temp_ratio = 0\n",
    "                        for k in range(len(temp_list)): # for each neighbour value\n",
    "                            # print(temp_list[k])\n",
    "                            temp_result = temp_result + temp_list[k] * (temp_normal - temp_dist[k])\n",
    "                            temp_ratio = temp_ratio + (temp_normal - temp_dist[k])\n",
    "                            # print('is',temp_result/temp_ratio)\n",
    "                        temp_result = temp_result/temp_ratio # calculate the weighted average result, the weights are related to the distance of each point\n",
    "                        \n",
    "                        if(pre_nan):# if the previous value is empty\n",
    "                            keep_values[i, j] = temp_result\n",
    "                        else:# if the previous value exists\n",
    "                            pre_value = keep_values[i-1, j]\n",
    "                            if(pre_value>0 and temp_result>0):# calculate the change% between the previous value and the weighted average result\n",
    "                                change_percent = (temp_result - pre_value) / pre_value\n",
    "                                if(change_percent<0):# if the previous value bigger than the weighted average result\n",
    "                                    change_percent = - change_percent\n",
    "                                    if((pre_value<20 and change_percent < normal_change_health[j])\n",
    "                                      or(pre_value>=20 and change_percent < normal_change_else[j])):\n",
    "                                        keep_values[i, j] = temp_result\n",
    "                                    else:# if the weighted average result is smaller than the normal situation\n",
    "                                        if(pre_value<20):\n",
    "                                            keep_values[i, j] = pre_value * (1 - normal_change_health[j]) \n",
    "                                        else:\n",
    "                                            keep_values[i, j] = pre_value * (1 - normal_change_else[j]) \n",
    "                                else:# if the previous value smaller than the weighted average result\n",
    "                                    if((pre_value<20 and change_percent < normal_change_health[j])\n",
    "                                      or(pre_value>=20 and change_percent < normal_change_else[j])):\n",
    "                                        keep_values[i, j] = temp_result\n",
    "                                    else:# if the weighted average result is bigger than the normal situation\n",
    "                                        if(pre_value<20):\n",
    "                                            keep_values[i, j] = pre_value * (1 + normal_change_health[j]) \n",
    "                                        else:\n",
    "                                            keep_values[i, j] = pre_value * (1 + normal_change_else[j])   \n",
    "                            else:\n",
    "                                keep_values[i, j] = temp_result\n",
    "                    else:# if all of the neighbour values are no empty\n",
    "                        pass         \n",
    "                else:# this value is no empty\n",
    "                    this_nan = False\n",
    "                pre_nan = this_nan\n",
    "        station_group[name].loc[:, attr_need] = keep_values\n",
    "        keep_ = station_group[name][attr_need]\n",
    "        print(keep_.isnull().sum())\n",
    "    \n",
    "attr_need = [\"NOx_Concentration\", \"NO2_Concentration\"]\n",
    "neighbour_padding(station_group, attr_need, station_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gade\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "Tag\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# padding the NaN value with the average value of the previous value and the next value\n",
    "\n",
    "def time_padding(station_group, attr_need):\n",
    "    for name in station_position.keys():# padding the nan value\n",
    "        print(name)\n",
    "        keep_ = station_group[name][attr_need]\n",
    "        print(keep_.isnull().sum())\n",
    "        keep_values = keep_.values           \n",
    "        \n",
    "        for j in range(keep_values.shape[1]):\n",
    "            pre_nan = True\n",
    "            for i in range(keep_values.shape[0]):\n",
    "                if np.isnan(keep_values[i, j]):# this value is empty\n",
    "                    this_nan = True\n",
    "                    if(i+1 < keep_values.shape[0]):\n",
    "                        if(not np.isnan(keep_values[i+1, j])):# next value exists\n",
    "                            if(not pre_nan):# previous value  exists\n",
    "                                temp_result = (keep_values[i+1, j] + keep_values[i-1, j]) / 2\n",
    "                            else:# no previous value\n",
    "                                temp_result = keep_values[i+1, j]\n",
    "                            keep_values[i, j] = temp_result\n",
    "                    else:# no next value\n",
    "                        if(not pre_nan):# previous value  exists\n",
    "                            keep_values[i, j] = keep_values[i-1, j]\n",
    "                        \n",
    "                else:# this value is no empty\n",
    "                    this_nan = False\n",
    "                pre_nan = this_nan\n",
    "                \n",
    "        station_group[name].loc[:, attr_need] = keep_values\n",
    "        keep_ = station_group[name][attr_need]\n",
    "        print(keep_.isnull().sum())\n",
    "    \n",
    "attr_need = [\"NOx_Concentration\", \"NO2_Concentration\"]\n",
    "time_padding(station_group, attr_need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2352443608470127 0.8912853213303326\n",
      "0.23783715066543912 0.8850931677018633\n",
      "0.2793970525986878 0.9356091247370976\n",
      "0.18661335342386087 0.8498766954377311\n",
      "Gade\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "Tag\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n",
      "NOx_Concentration    0\n",
      "NO2_Concentration    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# second time of padding the NaN value with neighbour values and historical values\n",
    "\n",
    "attr_need = [\"NOx_Concentration\", \"NO2_Concentration\"]\n",
    "neighbour_padding(station_group, attr_need, station_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(\"Aal/air_processing.pkl\", station_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>NO2_Concentration</th>\n",
       "      <th>NOx_Concentration</th>\n",
       "      <th>time_year</th>\n",
       "      <th>time_month</th>\n",
       "      <th>time_week</th>\n",
       "      <th>time_day</th>\n",
       "      <th>time_hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-06 10:00:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>11.84</td>\n",
       "      <td>18.23</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06 10:30:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>17.29</td>\n",
       "      <td>26.09</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06 11:00:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>21.40</td>\n",
       "      <td>31.80</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06 11:30:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>16.03</td>\n",
       "      <td>28.80</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06 12:00:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>25.11</td>\n",
       "      <td>38.67</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 08:00:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>7.27</td>\n",
       "      <td>14.39</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 08:30:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>8.90</td>\n",
       "      <td>17.27</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 09:00:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>9.32</td>\n",
       "      <td>19.16</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 09:30:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>8.00</td>\n",
       "      <td>19.38</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 10:00:00</th>\n",
       "      <td>Gade</td>\n",
       "      <td>8.90</td>\n",
       "      <td>20.69</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10897 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    station_id  NO2_Concentration  NOx_Concentration  \\\n",
       "time                                                                   \n",
       "2019-12-06 10:00:00       Gade              11.84              18.23   \n",
       "2019-12-06 10:30:00       Gade              17.29              26.09   \n",
       "2019-12-06 11:00:00       Gade              21.40              31.80   \n",
       "2019-12-06 11:30:00       Gade              16.03              28.80   \n",
       "2019-12-06 12:00:00       Gade              25.11              38.67   \n",
       "...                        ...                ...                ...   \n",
       "2020-07-20 08:00:00       Gade               7.27              14.39   \n",
       "2020-07-20 08:30:00       Gade               8.90              17.27   \n",
       "2020-07-20 09:00:00       Gade               9.32              19.16   \n",
       "2020-07-20 09:30:00       Gade               8.00              19.38   \n",
       "2020-07-20 10:00:00       Gade               8.90              20.69   \n",
       "\n",
       "                     time_year  time_month  time_week  time_day  time_hour  \n",
       "time                                                                        \n",
       "2019-12-06 10:00:00       2019          12          4         6         10  \n",
       "2019-12-06 10:30:00       2019          12          4         6         10  \n",
       "2019-12-06 11:00:00       2019          12          4         6         11  \n",
       "2019-12-06 11:30:00       2019          12          4         6         11  \n",
       "2019-12-06 12:00:00       2019          12          4         6         12  \n",
       "...                        ...         ...        ...       ...        ...  \n",
       "2020-07-20 08:00:00       2020           7          0        20          8  \n",
       "2020-07-20 08:30:00       2020           7          0        20          8  \n",
       "2020-07-20 09:00:00       2020           7          0        20          9  \n",
       "2020-07-20 09:30:00       2020           7          0        20          9  \n",
       "2020-07-20 10:00:00       2020           7          0        20         10  \n",
       "\n",
       "[10897 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_group_test = read_pkl(\"Aal/air_processing.pkl\")\n",
    "station_group['Gade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_id           0\n",
      "NO2_Concentration    0\n",
      "NOx_Concentration    0\n",
      "time_year            0\n",
      "time_month           0\n",
      "time_week            0\n",
      "time_day             0\n",
      "time_hour            0\n",
      "dtype: int64\n",
      "station_id           0\n",
      "NO2_Concentration    0\n",
      "NOx_Concentration    0\n",
      "time_year            0\n",
      "time_month           0\n",
      "time_week            0\n",
      "time_day             0\n",
      "time_hour            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if all values exist\n",
    "for name in station_group_test:\n",
    "    print(station_group_test[name].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ceb70875582159136b9c7d207ebd2016435d2a136370e35b97c5e7cd11b8213"
  },
  "kernelspec": {
   "display_name": "img (py36)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
